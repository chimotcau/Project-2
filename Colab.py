# -*- coding: utf-8 -*-
"""Bản sao của LAB1_np_pandas_matplotlib

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tw_wdRte-exL4tkVCIndofzprxLPvJU3
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# 1. Warm-up (4 points)

## 1.1 Creating Matrices (0.25 points)

Create 4 matrices
- A - a "checkerboard" matrix of zeros and ones, size 6x3, with the top-left value (A[0][0]) equal to 1,
- В - a matrix of numbers from 1 to 24, arranged in a snake-like pattern, size 6x4,
- C - a matrix of random integers from 2 to 10 (inclusive), size 4x3,
- D - a matrix of zeros with ones on the main diagonal, size 4x4.

**Create a “patchwork” matrix S from these matrices**

A В

С D

using **only standard** numpy operations. Do not use Comprehensions.

Then, append matrix **F** of size 10x2 filled with zeros to the resulting matrix **S** to create matrix **G**:

S F

Note: When we say a matrix has a size of x by y, x is the number of rows, and y is the number of columns.
"""

A = np.array([[1 if (i + j) % 2 == 0 else 0 for j in range(3)] for i in range(6)])
B = np.array([range(i * 4 + 1, i * 4 + 5) if i % 2 == 0 else range(i * 4 + 4, i * 4, -1) for i in range(6)])
C = np.random.randint(2, 11, (4, 3))
D = np.eye(4, dtype=int)
S_top = np.hstack((A, B))
S_bottom = np.hstack((C, D))
S = np.vstack((S_top, S_bottom))
F = np.zeros((10, 2), dtype=int)
G = np.hstack((S, F))
print("Matrix A:\n", A)
print("\nMatrix B:\n", B)
print("\nMatrix C:\n", C)
print("\nMatrix D:\n", D)
print("\nPatchwork Matrix S:\n", S)
print("\nFinal Matrix G:\n", G)

"""## 1.2 Finding the Nearest Neighbor (0.25 points)

Implement a function that takes a matrix **X** and a number **a** and returns the element in the matrix closest to the given number.
   
For example, for **X = np.arange(0, 10).reshape((2, 5))** and **a = 3.6**, the answer will be 4. You can only use basic numpy functions, **do not use loops**.
"""

def find_nearest_neighbour(X, a):
    differences = np.abs(X - a)
    nearest_index = np.argmin(differences)
    return X.flat[nearest_index]
X = np.arange(0, 10).reshape((2, 5))
a = 3.6
result = find_nearest_neighbour(X, a)
print("Closest element:", result)

"""## 1.3 Very Strange Neural Network (0.25 points)

Implement a strange neural network. The network should:

- Square matrix **A** (the weight matrix) of size N x N.
- In the first transformation, multiply a vector **X** of length N (feature vector) by the weight matrix **A^2** (the output will be a new vector).
- In the second transformation, multiply the resulting vector by vector **b** (weight vector) of size N to produce a scalar value.

Assume that all elements in matrices and vectors are floating-point numbers.
"""

N = 4
A = np.random.rand(N, N)
b = np.random.rand(N)
X = np.random.rand(N)
def very_strange_neural_network(A, b, X):
    A_squared = np.dot(A, A)
    transformed_vector = np.dot(A_squared, X)
    result = np.dot(transformed_vector, b)
    return result
output = very_strange_neural_network(A, b, X)
print("Output of the strange neural network:", output)

"""## 1.4 The Jungle Calls! (0.25 points)

You are given a matrix **M**, a map of an impassable jungle terrain created by Lara Croft. Each cell in the map is an integer representing the height above sea level (if positive) in meters or the sea depth (if negative) in meters in a one-meter-by-one-meter area of the map. If the number is 0, it represents land - a shoreline.

You need to calculate:
- The total area of cells in the sea where the depth is greater than 5 (in m^2).
- The total volume of water on the map (in m^3).
- The maximum height above sea level on this map (in m).
"""

def find_deep_sea_area(M):
    deep_sea = M < -5
    return np.sum(deep_sea)

def find_water_volume(M):
    water_depth = np.abs(M[M < 0])
    return np.sum(water_depth)
def find_max_height(M):

    return np.max(M)

# You can create your own example.
M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])

# simple check for the example above
assert np.isclose(find_deep_sea_area(M), 1)
assert np.isclose(find_water_volume(M), 21)
assert np.isclose(find_max_height(M), 25)

print("Total sea area on the map -", find_deep_sea_area(M), "м^2")
print("Total water volume on the map -", find_water_volume(M), "м^3")
print("Maximum elevation on the map -", find_max_height(M), "м")

"""## 1.5 Treasure Islands (0.25 points)


The function takes an array **a** of zeros and ones as input. Count the number of consecutive blocks of ones (islands) in the array. Only basic numpy functions are allowed, **no loops**.

Hint: check what `np.diff` does.
"""

def count_all_islands(a):
    diff = np.diff(a)
    starts = np.sum(diff == 1)
    return starts + (a[0] == 1)

a = np.array([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1])

assert count_all_islands(a) == 4
print(count_all_islands(a))

"""## 1.6 Masquerade (0.25 points)

The input is a 2D matrix **X** filled with floating-point numbers and a floating-point number **a**. Replace all cells in the matrix greater than **a** with the average value of the elements in **X**.

**You must** use basic numpy functions, **no loops**.
"""

def swap_mask_for_average(X, a):
    avg = np.mean(X)
    mask = X > a
    X[mask] = avg
    return X

M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])
a = 5

result = swap_mask_for_average(M.copy(), a)
print(result)

"""## 1.7 Hot on the Trails (0.25 points)

The input is a square matrix **M**. Calculate the difference between the sum along the main diagonal and the secondary diagonal.

Only basic numpy functions are allowed, **no loops**.

Hint: look up `np.trace`.
"""

def count_trace_diff(M):
    main_diag_sum = np.trace(M)
    secondary_diag_sum = np.trace(np.fliplr(M))
    return main_diag_sum - secondary_diag_sum

M = np.array([
    [-7, -3, -1, 0],
    [-4, -3, 1, 19],
    [-2, 0, 4, 25],
    [-1, 3, 6, 9]
])

result = count_trace_diff(M)
print(result)

"""## 1.8 King of the Hill (0.25 points)

The input is a vector a of size N. Using addition, concatenation, and broadcasting, create a symmetric matrix of size 2N x 2N with the maximum value in the center and decreasing values toward the edges.

Example: a = (0, 1, 2)

Result:

0 1 2 2 1 0 \\
1 2 3 3 2 1 \\
2 3 4 4 3 2 \\
2 3 4 4 3 2 \\
1 2 3 3 2 1 \\
0 1 2 2 1 0 \\
"""

def create_mountain(a):
    extended = np.concatenate((a, a[::-1]))
    mountain = extended[:, None] + extended[None, :]
    return mountain

a = np.array([0, 1, 2, 3, 4])
mountain_matrix = create_mountain(a)

print(mountain_matrix)

"""## 1.9 Monochrome Photograph 9x12 (0.5 points)

The input is a 2D matrix **P** of size N x M filled with numbers from 0 to 255, representing a grayscale photograph, and a natural number **C**. You need to produce a matrix of size (N - C + 1) x (M - C + 1), where each cell is the average value of the corresponding sub-matrix of size **C x C**. Essentially, this will apply a simple blur effect (slightly reducing its size).
"""

def custom_blur(P, C):
    N, M = P.shape
    result_shape = (N - C + 1, M - C + 1)
    blurred = np.zeros(result_shape)
    for i in range(result_shape[0]):
        for j in range(result_shape[1]):
            blurred[i, j] = np.mean(P[i:i+C, j:j+C])
    return blurred

P = np.arange(0, 12).reshape((3, 4))
kernel = 2

blurred = custom_blur(P, kernel)
print("Blurred Matrix:\n", blurred)
assert np.allclose(blurred, np.array([[2.5, 3.5, 4.5], [6.5, 7.5, 8.5]]))

"""## 1.10 Validation Function (0.75 points)

The input to the function is an arbitrary number (>2) of tuples representing the shapes of different matrices. The function should return True if the matrices can be sequentially added together (possibly using broadcasting), and False otherwise.
"""

def check_successful_broadcast(*matrices):
    current_shape = matrices[0]
    for next_shape in matrices[1:]:
        result_shape = []
        for dim1, dim2 in zip(reversed(current_shape), reversed(next_shape)):
            if dim1 == dim2 or dim1 == 1 or dim2 == 1:
                result_shape.append(max(dim1, dim2))
            else:
                return False
        if len(current_shape) > len(next_shape):
            result_shape.extend(reversed(current_shape[:-len(next_shape)]))
        else:
            result_shape.extend(reversed(next_shape[:-len(current_shape)]))
        current_shape = tuple(reversed(result_shape))
    return True

assert check_successful_broadcast((5, 6, 7), (6, 7), (1, 7))
assert not check_successful_broadcast((5, 6, 7), (8, 6, 7))

print(check_successful_broadcast((5, 6, 7), (6, 7), (1, 7)))
print(check_successful_broadcast((5, 6, 7), (8, 6, 7)))

"""## 1.11 Pairwise Distances (0.75 points)

The input is matrices A of size m x k and B of size n x k. Create a matrix of size m x n containing pairwise Euclidean distances.

Only use basic functions, do not use loops or third-party libraries. Broadcasting will probably be useful. The solution must be **in one line**, following all code style rules.
"""

def pairwise_distances(A, B):
    return np.sqrt(np.sum((A[:, None, :] - B[None, :, :])**2, axis=-1))

A = np.array([[1, 2], [3, 4], [5, 6]])
B = np.array([[7, 8], [9, 10]])

print(pairwise_distances(A, B))

"""Explain the logic behind this one-liner. What exactly is happening?

<font color='red'> YOUR ANSWER HERE </font>

# 2. Data Experiment Processing (3 points)

Ladies and gentlemen, now we're going to learn to use libraries for data analysis in real-life scenarios!

**The reason behind this section is simple**: many students in the Faculty of Physics and Mathematics still rely on Excel, calculators, or pen and paper even in their second or third semesters. Our goal is to introduce another method for conducting laboratory work with a much lower entry barrier than Excel itself. We hope this motivates some to explore these handy libraries further.

*Data sponsor for this section - blacksamorez. Without them, five happy semesters of labs would have been far less joyful...*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""##  2.1. Problem Statement and Data

Let's assume we have a gyroscope with a weight attached to its axis on an arm (see the diagram for a quick understanding, and you can find more details in the [laboratory manual](https://lib.mipt.ru/book/267519/), volume 1, p.160). Due to the weight, the gyroscope begins to slowly [precess](https://en.wikipedia.org/wiki/Precession), i.e., it rotates around the vertical axis with a relatively constant frequency.

We'll work through part of this lab, primarily focusing on data processing and plotting graphs.

<center><img src='https://drive.google.com/uc?export=view&id=1KfYQ0hKYRDhi5uk7C8lNffZBNy8NF7nu' width=600>

Diagram of the gyroscope with the attached weight G and arm C</center>

First, let's examine the data someone has kindly gathered for us. Create a dataframe from [this file](https://drive.google.com/file/d/1SbLR6R16obqLewUTnX1CAAjQTrVXh2Vq/view?usp=sharing)
"""

file_url = "https://drive.google.com/uc?export=download&id=1SbLR6R16obqLewUTnX1CAAjQTrVXh2Vq"
data = pd.read_csv(file_url)
print(data.head())

"""## 2.2 Working with Data

The columns in the dataframe are as follows:

- N: Number of full gyroscope rotations in the experiment;
- t, in seconds: Time of the experiment;
- σ_t: Measurement error in time;
- mass: Mass of the weight attached to the arm on the gyroscope;
- length: Length of the previously mentioned arm;
- phi: Angle in radians by which the arm dropped during the experiment. This will help us estimate the effect of friction in the gyroscope on precession.

Since physicists like to work with properly dimensioned quantities, convert the mass columns to kilograms and length to meters. Then rename all columns to exclude references to units—use only the names of physical quantities.
"""

data['mass'] = data['mass, gramm'] / 1000
data['length'] = data['length, cm'] / 100

data = data.rename(columns={
    'N': 'N',
    't, sec': 'time',
    'sigma_t, sec': 'sigma_time',
    'phi, rad': 'phi',
})
data = data[['N', 'time', 'sigma_time', 'mass', 'length', 'phi']]
data.head()

# @title N

from matplotlib import pyplot as plt
data['N'].plot(kind='hist', bins=20, title='N')
plt.gca().spines[['top', 'right',]].set_visible(False)

"""Add new columns to the dataframe with the corresponding names and values, calculated using these formulas:

`omega`: $\Omega = 2 \pi \cdot \frac{N}{t}$

`sigma_omega`: $\sigma_{\Omega} = \Omega / t \cdot \sigma_t$

`omega_down`: $\Omega_{down} = \varphi / t$

`sigma_down`: $\Omega_{down} \cdot \sigma_t / t$

`momentum`: $M = m \cdot g \cdot l$ (`g = 9.8 м/с^2`)

`momentum_down`: $M_{down} = m \cdot \frac{\varphi}{t^2} \cdot l^2$

`sigma_momentum`: $\sigma_{M} = M_{down} \cdot 2 \cdot \frac{\sigma_t}{t}$

"""

g = 9.8


data['omega'] = 2 * np.pi * data['N'] / data['time']
data['sigma_omega'] = (data['omega'] / data['time']) * data['sigma_time']
data['omega_down'] = data['phi'] / data['time']
data['sigma_down'] = data['omega_down'] * data['sigma_time'] / data['time']
data['momentum_down'] = (data['mass'] * (data['phi'] * data['time']**2) / (data['length']**2))

data['momentum_down'] = data['mass'] * (data['phi'] * data['time']**2) / (data['length']**2)
data['sigma_momentum'] = data['momentum_down'] * 2 * data['sigma_time'] / data['time']



data.head()

"""You may have already wondered why the experiments with the same mass are repeated so many times. To achieve more stable results, of course! Now calculate the average values of the columns `omega`, `σ_omega`, `momentum`, and `momentum_down` for each unique mass.


**Hint:** The groupby function will help you here. No loops allowed!
"""

grouped_data = data.groupby('mass')[['omega', 'sigma_omega', 'momentum', 'momentum_down']].mean()
assert 0.273 in grouped_data.index
assert np.allclose(grouped_data.omega[0.273], 0.1433)
a
grouped_data

# @title momentum vs momentum_down

from matplotlib import pyplot as plt
grouped_data.plot(kind='scatter', x='momentum', y='momentum_down', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""## 2.3 Simple Graphs and Least Squares Method (LSM)

It's now time to reintroduce the Least Squares Method (LSM). Of course, we won't make you write LSM yourself! <s>We're not that cruel</s>

In NumPy, the function [np.polyfit](https://numpy.org/devdocs/reference/generated/numpy.polyfit.html) calculates a polynomial of a given degree that best fits `y(x)` using LSM for the provided `x`, `y`, and degree `p`.

The function [np.polyval](https://numpy.org/devdocs/reference/generated/numpy.polyval.html) evaluates the polynomial `P(x)` with given coefficients.

Your task is to plot the dependence $\Omega (M)$ (angular velocity on moment of inertia). The graph should include experimental points and a line fitted using the least squares method. In the legend, include the polynomial with the calculated coefficients. Don't forget to label the axes (font size 14), add a grid, and provide a suitable title (font size 18)!

<center><img src='https://drive.google.com/uc?export=view&id=1xumON0195iA4HGSqvpS0FAhPGxuCdKH8' width=600>

Example of the resulting graph</center>
"""

omega_np = np.array(grouped_data.omega)
momentum_np = np.array(grouped_data.momentum)

coefs = np.polyfit(momentum_np, omega_np, 1)

x_lsq = np.linspace(momentum_np.min() * 0.5, momentum_np.max() * 1.1, 100)
y_lsq = np.polyval(coefs, x_lsq)

fig = plt.figure(figsize=(12, 8))
plt.scatter(momentum_np, omega_np, color='blue', label='Experimental points', zorder=5)
plt.plot(x_lsq, y_lsq, color='red', label=f'Fit: $y = {coefs[0]:.2f}x + {coefs[1]:.2f}$', zorder=4)

plt.xlabel('Momentum (N·m)', fontsize=14)
plt.ylabel('Angular velocity $\Omega$ (rad/s)', fontsize=14)
plt.title('Dependence of Angular Velocity on Momentum', fontsize=18)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

"""`np.polyfit` also estimates errors! Specifically, it returns the covariance matrix for the least squares method. Without delving into the math, all you need to know is that the diagonal elements are the variances of the calculated coefficients. To get the actual error $\sigma$, take the square root of these variances.

Also, note the `W` parameter, which sets point weights for the estimate. If $y_{error}$ values are known, you can set weights as $W = 1 / y_{error}$ to get an even more precise line. To account for $x$ errors as well, you'd need other methods (but $y$ errors alone are likely sufficient).

Suppose there's been a mishap, and the errors have increased tenfold!
"""

grouped_data['sigma_down'] *= 10
grouped_data['sigma_momentum'] *= 10

"""Now, you need to plot the dependence $\Omega_{down} (M_{down})$ <b>(not $\Omega(M)$!)</b> for points with error bars. In addition to plotting the line from the least squares estimate, include the error estimates for the coefficients. So, plot three lines: $k \cdot x + b$ from the LSM, $(k - \sigma_k) \cdot x + (b - \sigma_b)$, and $(k + \sigma_k) \cdot x + (b + \sigma_b)$, filling the area between these lines with shading (use `plt.fill_between` for this). Keep the rest of the styling as in the previous task.

_Note: Often in LSM, only the error for `k` is considered, leaving out `σ_b`._

<center><img src='https://drive.google.com/uc?export=view&id=1SriaMzJah7F610ocIK_O1-HqqtMQgxlg' width=600>

Example of the resulting graph</center>
"""



"""# 3. Working with the Dataset (3 points)

The Iris dataset was used in R.A. Fisher's 1936 paper “The Use of Multiple Measurements in Taxonomic Problems” and is now commonly used by beginner data analysts.

The dataset includes three species of iris flowers, with 50 samples for each species, along with several properties of each flower. One species is linearly separable from the other two, but the latter two are not linearly separable from each other.

The columns in this dataset:

Identifier (Id) \\
Sepal length in cm (SepalLengthCm) \\
Sepal width in cm (SepalWidthCm) \\
Petal length in cm (PetalLengthCm) \\
Petal width in cm (PetalWidthCm) \\
Species (Species) \\

<font color='red'>Attention!</font> All plots in this part should be labeled!
"""

sns.set_style("darkgrid")

iris = sns.load_dataset('iris')
iris.head()

iris.columns # There is no ID column in the table

"""Let's check how many different iris species we have—there should be three, with 50 samples each. Use `value_counts` to see the possible values in the species column."""

species_counts = iris['species'].value_counts()
species_counts

"""## 3.1 Petal Length and Width Plots

Let's examine whether petal width and length are related—use `sns.scatterplot` to display the OXY values. Remember to label the plot and the axes!
"""

sns.scatterplot(data=iris, x='petal_length', y='petal_width', hue='species', palette='deep')

plt.title('Petal Length vs Petal Width', fontsize=18)
plt.xlabel('Petal Length (cm)', fontsize=14)
plt.ylabel('Petal Width (cm)', fontsize=14)

plt.show()

"""What conclusions can you draw from the scatter plot? What is missing from the plot to make it more informative?

<font color='red'>YOUR ANSWER HERE</font>
What conclusions can you draw from the scatter plot?

<font color='white'>From the scatter plot of petal length vs petal width, we can draw the following conclusions:

1.  The plot clearly shows that Setosa species is well-separated from the other two species (Versicolor and Virginica) based on both petal length and petal width.
2.   There appears to be a positive correlation between petal length and petal width.

What is missing from the plot to make it more informative?
*   Axes Units
*   Gridlines

*  Point Sizes or Transparency

Let's try other plot types for the same purpose from the seaborn library—`sns.jointplot`, and also try to color the points based on iris species using `sns.facetgrid`. Display these two plots in the following cells and analyze them.
"""

joint_plot = sns.jointplot(x='petal_length', y='petal_width', data=iris, kind="scatter", hue="species", palette="Set1")

joint_plot.fig.suptitle('Petal Length vs Petal Width', fontsize=18)
joint_plot.fig.tight_layout()
joint_plot.fig.subplots_adjust(top=0.95)
plt.show()

facet_grid = sns.FacetGrid(iris, col="species", height=5, aspect=1.2)
facet_grid.map(sns.scatterplot, 'petal_length', 'petal_width')

facet_grid.set_axis_labels('Petal Length (cm)', 'Petal Width (cm)', fontsize=14)
facet_grid.set_titles('{col_name}', fontsize=16)
plt.show()



"""What conclusions can you draw from these plots? Which of the three methods — scatterplot, jointplot, or facetgrid seems the best to you?

<font color='red'> YOUR ANSWER HERE </font> FacetGrid seems the best method in this case because it allows you to visually compare the relationship between petal_length and petal_width across different species. This is crucial since one of the main objectives here is to understand how each species behaves in relation to the same variables.

## 3.2 Distribution Plots of Petal Length Values

Plot a “box-and-whisker plot” with `sns.boxplot` and its counterpart with `sns.violinplot`. The x-axis should represent the iris species, and the y-axis should represent petal length.
"""

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.boxplot(data=iris, x='species', y='petal_length')
plt.title('Boxplot of Petal Length by Species', fontsize=16)
plt.xlabel('Iris Species', fontsize=14)
plt.ylabel('Petal Length (cm)', fontsize=14)

plt.subplot(1, 2, 2)
sns.violinplot(data=iris, x='species', y='petal_length')
plt.title('Violin Plot of Petal Length by Species', fontsize=16)
plt.xlabel('Iris Species', fontsize=14)
plt.ylabel('Petal Length (cm)', fontsize=14)

plt.tight_layout()
plt.show()



"""Compare the two types of plots. Which one is more informative, and which is more visually appealing in your opinion? What information can we gather from these plots?

<font color='red'> YOUR ANSWER HERE </font>Both plots show that the Setosa species has a distinctively smaller petal length, with most of the data clustered at lower values.
The Versicolor and Virginica species have overlapping distributions, but the Virginica species tends to have a larger spread and higher petal lengths, as seen in both the boxplot and violin plot.
The Setosa species shows a more concentrated distribution with a small interquartile range, while Versicolor and Virginica show wider distributions.

## 3.3 Pairwise Feature Comparison Plots


Let's create a 4x4 grid of plots where all possible pairs of features (petal length/width, sepal length/width) are displayed. Use `sns.pairplot`, and don't forget to specify the `hue` parameter.
"""

iris = sns.load_dataset('iris')
sns.pairplot(iris, hue='species')
plt.show()

"""What information can you gather about the feature relationships from this plot?

<font color='red'> YOUR ANSWER HERE </font>Most informative plots: The scatter plots between petal length, petal width, and sepal length show clear separations or patterns that help distinguish the species.
Most visually appealing: The pairplot gives a good overview of the feature relationships in one glance, but separate scatter plots or violin plots might give you more specific insights on certain pairwise relationships.

What plots are on the diagonal of this grid?

<font color='red'> YOUR ANSWER HERE </font>histograms for each feature.

Try replacing the diagonal plots with potentially more informative ones (hint: `sns.pairplot` has a special parameter for this in the documentation). Display the resulting plot.
"""

sns.pairplot(iris, hue='species', diag_kind='kde')
plt.show()

"""What plots are now on the diagonal? Does it seem more informative now?

<font color='red'> YOUR ANSWER HERE </font>Distribution of each feature individually for each species.This setup does seem more informative as it highlights the individual feature distributions while still showing the pairwise relationships between features off the diagonal.
"""